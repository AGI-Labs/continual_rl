[
    {"policy": "ppo", "experiment": "mini_atari_cycle"},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "num_steps": 128},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "num_processes": 8, "clip_param": 0.1, "learning_rate": 2.5e-4, "num_steps": 128, "ppo_epoch": 3, "value_loss_coef": 1, "max_grad_norm": 10},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "num_processes": 8, "clip_param": 0.1, "learning_rate": 2.5e-4, "num_steps": 128, "ppo_epoch": 3, "value_loss_coef": 1, "max_grad_norm": 10, "comment": "Full Atari reward"},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "num_processes": 8, "clip_param": 0.1, "learning_rate": 2.5e-4, "use_gae": true, "num_steps": 128, "value_loss_coef": 0.5, "num_mini_batch": 4, "comment": "Full Atari reward"},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "use_linear_lr_decay": true, "num_processes": 8, "clip_param": 0.1, "learning_rate": 2.5e-4, "use_gae": true, "num_steps": 128, "value_loss_coef": 0.5, "num_mini_batch": 4, "comment": "Full Atari reward"},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "use_linear_lr_decay": true, "decay_over_steps": 1000000, "num_processes": 8, "clip_param": 0.1, "learning_rate": 2.5e-4, "use_gae": true, "num_steps": 128, "value_loss_coef": 0.5, "num_mini_batch": 4, "comment": "Full Atari reward"},
    {"policy": "ppo", "experiment": "pong", "use_linear_lr_decay": true, "decay_over_steps": 10000000, "num_processes": 8, "clip_param": 0.1, "learning_rate": 2.5e-4, "use_gae": true, "num_steps": 128, "value_loss_coef": 0.5, "num_mini_batch": 4, "comment": "Full Atari reward, fixed dist"}
]