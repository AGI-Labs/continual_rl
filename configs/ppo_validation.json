[
    {"policy": "ppo", "experiment": "mini_atari_cycle"},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "num_steps": 128},
    {"policy": "ppo", "experiment": "mini_atari_cycle", "num_processes": 8, "clip_param": 0.1, "learning_rate": 2.5e-4, "num_steps": 128, "ppo_epoch": 3, "value_loss_coef": 1, "max_grad_norm": 10}
]